%% REPLACE sXXXXXXX with your student number
\def\studentNumber{B241676}


%% START of YOUR ANSWERS
%% Add answers to the questions below, by replacing the text inside the brackets {} for \youranswer{ "Text to be replaced with your answer." }. 
%
% Do not delete the commands for adding figures and tables. Instead fill in the missing values with your experiment results, and replace the images with your own respective figures.
%
% You can generally delete the placeholder text, such as for example the text "Question Figure 2 - Replace the images ..." 
%
% There are 18 TEXT QUESTIONS (a few of the short first ones have their answers added to both the Introduction and the Abstract). Replace the text inside the brackets of the command \youranswer with your answer to the question.
%
% There are also 3 "questions" to replace some placeholder FIGURES with your own, and 3 "questions" asking you to fill in the missing entries in the TABLES provided. 
%
% NOTE! that questions are ordered by the order of appearance of their answers in the text, and not by the order you should tackle them. Specifically, you cannot answer Questions 2, 3, and 4 before concluding all of the relevant experiments and analysis. Similarly, you should fill in the TABLES and FIGURES before discussing the results presented there. 
%
% NOTE! If for some reason you do not manage to produce results for some FIGURES and TABLES, then you can get partial marks by discussing your expectations of the results in the relevant TEXT QUESTIONS (for example Question 8 makes use of Table 1 and Figure 2).
%
% Please refer to the coursework specification for more details.


%% - - - - - - - - - - - - TEXT QUESTIONS - - - - - - - - - - - - 

%% Question 1: Summarise why overfitting is a problem.
\newcommand{\questionOne} {
\youranswer
{In overfitting, the model learns too much about the training data and is unable to generalise its 
learnings for unseen data which leads to poor performance on new data points.}
}

%% Question 2: Summarise the effect increasing width and depth of the architecture had on overfitting.
\newcommand{\questionTwo} {
\youranswer
{Increasing the depth and width slightly increases the model performance overall. If either or both the width
and depth are increased by a lot, the model starts overfitting the training data and returns low accuracy scores for any unseen data.}
}

%% Question 3: Summarise what your results show you about the effect of the tested approaches on overfitting and the performance of the trained model
\newcommand{\questionThree} {
\youranswer
{that the model which was just using dropout regularisation was performing the best. It was able to properly learn from the training data and returned high accuracy scores on the testing data while keeping the error of the model low.}
}

%% Question 4: Give your overall conclusions
\newcommand{\questionFour} {
\youranswer
{complex models can easily overfit the data resulting in low accuracy scores on unseen data points. We can use various techniques like Dropout \& L1/L2 regularisations to ensure that the model doesn't memorise the data and is instead able to generalise the learnings from it.}
}

%% Question 5: Explain in detail, and in your own words, why overfitting is a problem
\newcommand{\questionFive} {
\youranswer
{Overfitting causes a model to start recognising random deviances and noise present in the training data as a regularity rather than learning the underlying patterns.
This poor generalisation leads to the model struggling to capture the trends on unseen data points and hence performing poorly on them. This makes the model less reliable to be used in real world applications.
Another direct consequence of overfitting is that it skews the relationship between variables affecting the interpretability of the model and making it difficult to fix the model.
Overfitting a model also wastes a lot of resources as it would require a lot of computational power to create a model which won't be able to handle new data points very well.}
}

%% Question 6: Explain the relevance of ``network capacity'' to overfitting
\newcommand{\questionSix} {
\youranswer
{Network capacity refers to the ability of the model to learn the general characteristics of the data. When a model is more complex, it starts memorising the data rather than learning the trends from it.
This usually happens when the model has more room than what is needed for capturing the patterns in the data. If the dataset is small or simple, the basic essense of it can be captured with a small model itself.
If in scenarios like this, a complex model is used, the model has the capacity to learn more than the general features of the data and will also start to fit the outliers in the data leading to overfitting on the training set.
Thus it is very important to choose an appropriate model for the data given to us otherwise choosing a complex model with more network capacity will result in overfitting of the data.}
}

%% Question 7: Explain what these figures contain and how the curves evolve, and spot where overfitting occurs. Reason based on the min/max points and velocities (direction and magnitude of change) of the accuracy and error curves
\newcommand{\questionSeven} {
\youranswer
% Describe figure 1a
% all the different width models give very similar accuracy. The models increase the training and validation accuracy rapidly in the first few epochs. After the inital jump, the lines flatten out and marginal improvement is seen in the training accuracy of all the models. On the other hand, 
% the validation accuracy remains similar and doesn't improve much. This is where the overfitting has started, signifying that the model has learned all it could from the training data and now its trying to fit the noise in the data. \par This is further supported 
% by Figure 1b, where we also see the error drop at a rapid rate with the first few epochs. But as the epochs increase, the error on the validation set starts increasing signifying that the model has started to memorise the training data rather than generalising the learnings from it.
{the training accuracy of the model increases rapidly in the first few epochs and then gradually slows down as the epochs continue to run. Even though the validation accuracy also increases at a good rate in the first few epochs, it starts flattening out at a point and then slowly starts to 
decline. This is the point where overfitting has started, stating that the model has learned the training data and is unable to generalise it to the validation data. This is further validated by the graph in Figure 1b which plots the error of the model on the training and validation dataset. 
We can see that the training error reduces rapidly at first and then eventually tends to 0. After a rapid initial decline in the error of the validation set, the error starts increasing at the same time when the accuracy of the validation set starts decreasing. This is due to the fact the model has now overfit 
and is struggling to process unseen data points.}
}

%% Question 8: Present your network width experiment results by using the relevant figure and table
\newcommand{\questionEight} {
\youranswer
{the final validation accuracy of all the models is very close (Figure 2a) indicating that the models performance very similarly. In Figure 2b, if we look at the train and the validation error, we can see the difference in the performance of the models. The training error of the model with 32 hidden units is the highest 
and the one with 128 hidden units is the lowest. This would ideally mean that the model with 128 neurons would be perform the best. But if we look at the validation error, we can see the 128 units model has the highest validation error. This means that model has overfit the training data and is performing relatively
worse on data it has not seen before when compared to the 32 units model. \par From Table 2, we can see that the model with 64 hidden units has the highest validation accuracy and the lowest validation error signifying that the model has learned from the training data and is able to generalise the best on unseen data out of all the 3 models. 
This model is relatively not too complex and is performing better than a model with more width.}
}

%% Question 9: Discuss whether varying width affects the results in a consistent way, and whether the results are expected and match well with the prior knowledge (by which we mean your expectations as are formed from the relevant Theory and literature)
\newcommand{\questionNine} {
\youranswer
{Changing the width of the model gave us a variety of results. We saw that a model with more width (more complex model) is not necessarily the best model for most scenarios. Even though the model might slightly perform better on the training data, it gives a higher error and not a significant increase on the accuracy for new data points.
The results we've achieved also prove the point about the network capacity, where a higher width model has the more than the required capacity to learn the features of the data and thus it starts fitting the outliers to the data leading to overfitting. This won't always be the case as this test was conducted on a relatively simple dataset.
If the data to be processed had more features, we would require a model with more width to learn the underlying patterns in the data. A complex model in this scenario won't overfit because its capacity is being completely utilised in learning the features of the data.}
}

%% Question 10: Present your network depth experiment results by using the relevant figure and table
\newcommand{\questionTen} {
\youranswer
{models of different depth are all capable of classifying classes of EMNIST dataset. From Figure 3a, providing the models with more depth has enabled them to extract features from the data efficiently and give excellent accuracy scores. Irrespective of their depth, in case of validation data, all of them are performing similarly and giving 
accuracy of around 80\% (Table 2). The rise of accuracy is very steep in the first 10 epochs, and then it is slow gradual improvement in training accuracy and barely any change in the validation accuracy. \par The error behaves very differently and gives a good understanding of the models. In Figure 3b, we can see that the error of all the models on 
the training set approaches 0, but on the validation set starts increasing as the epochs increase. This is the point of overfitting; and the magnitude of the error indicates how much the model has learned the training data. The model with the highest depth has the highest capacity and hence has been able to learn the train set the best resulting 
in it having the highest error on the validation set (from Table 2).}
}

%% Question 11: Discuss whether varying depth affects the results in a consistent way, and whether the results are expected and match well with the prior knowledge (by which we mean your expectations as are formed from the relevant Theory and literature)
\newcommand{\questionEleven} {
\youranswer
{Increasing the depth of the model can be helpful in processing large or complex datasets. But in other situations it can turn out to be counter productive and as the model will overfit the data. This is because certain capability is required for the model to process the data, but if the model has more room to capture features, it'll start fitting the 
noise to the dataset. As the depth of the model increases, there are more neurons available which allow the model to extract more features from the dataset allowing it to overfit the training data and result in poor accuracy scores for unseen data. It also becomes computationally expensive to train the model and the resources will be wasted if the model
overfits on the training data. We can tackle the problem of overfitting by using methods like Dropout and Regularisation which penalise the model for generalising too much to the training set preventing overfitting.}
}



%% Question 12: Explain how one could use a combination of L1 and L2 regularisation. Discuss any potential benefits of this approach
\newcommand{\questionTwelve} {
\youranswer{The L1 penalty also known as Lasso regularisation and L2 penalty known as Ridge regularisation can be used together to tackle the overfitting that occurs during the training of the model. One of the approaches to combining the L1 and L2 regularisations is called the Elastic Net. The equantion for the Elastic Net is given by:
\begin{equation}
    E = E_\text{data} + \lambda_1 \sum_{i=1}^N |w_i| + \lambda_2 \sum_{i=1}^N w_i^2
\end{equation}
where $\lambda_1$ and $\lambda_2$ are the hyperparameters for the L1 and L2 regularisation respectively. \par The L1 regularisation is used to reduce the number of features by shrinking the coefficients of the less important features to 0. The L2 regularisation penalises the model for having large coefficients and shrinks them to a smaller scale.
By adjusting the values of $\lambda_1$ and $\lambda_2$ appropriately, we can find a right balance of L1 and L2 penalties that will help the model generalise over new or unseen data points reducing the overall variance and preventing overfitting. \par Sometimes while using L1 regularisation, 
the model can get stuck at a local minima due to some of the parameters being set to 0. This problem can be mitigated by using L2 norms along with L1 norms. This is because the L2 norm is a smooth function and hence the model can easily escape the local minima.}
}

%% Question 13: Explain the experimental details (e.g. hyperparameters), discuss the results in terms of their generalisation performance and overfitting. Select and test the best performing model as part of this analysis.
\newcommand{\questionThirteen} {
\youranswer
{We introduce a new hyperparameter for controlling the dropout in the model which is the inclusion probability. This hyperparameter specifies the percentage of the neurons that should be considered and the rest are converted to 0. This helps the model become less reliant on particular neurons and prevent overfitting. We tested with different inclusion 
probabilities ranging from 0.6 to almost 1 and found that the validation accuracy didn't increase much even though more neurons were available for the model to extract the patterns from the data. The validation accuracy increased at a slow pace with availability of more neurons, but the increase in generalisation gap was relatively high, hinting at 
overfitting of the model (Figure 4a).\par Using another hyperparameter, we controlled the amount of weight penalties that were applied to the model using the L1 \& L2 norms. In case of L1 penalties, as the coefficients of the penalty increased, the error increased while simultaneously decreasing the accuracy of the model. Since L1 penalties add the 
absolute values of the weights to the error, this told us that some features are relatively more important than others for prediction of the model. L2 penalties had a similar but not as extreme effect when compared to L1 penalties. Even though Figure 4b shows that the generalisation gap is decreasing, it is because the training error is also increasing 
signifying that the model is unable to fit the training data well.\par From these testings, we concluded that the dropout model with inclusion probability of 0.85 was performing the best. Even though the model with inclusion probability of 0.97 has a slightly higher validation accuracy, it comes at the cost of increased generalisation gap. This model 
was then used to classify the test set of the EMNIST data. The accuracy of the model on the test data is \textit{84.0\%} and the error is \textit{0.471}. This is because the model is able to generalise well over unseen data and has not overfit on the training set. We also noted that the model with 0.97 inclusion probability had a slightly higher accuracy 
on the test set of $84.55\%$. Although the accuracy is higher, this cannot be a factor in choosing a model because we can't check the accuracy scores on the test set in real life scenarios.}
}

%% Question 14: Assume you were able to run 8 further experiments (8 specific hyperparameter configurations) where you could combine Dropout and L1, and/or Dropout and L2 regularisation. Which 8 runs would you pick and what question(s) would you aim to answer? Make sure you define the experiment setup, including any relevant hyperparameters
\newcommand{\questionFourteen} {
\youranswer
{We can extend this experiment by testing a plethora of combinations of the dropout regularisation and L1 \& L2 penalties to fine tune the model further. We have already seen that each regularisation method is used for a specific task and has its own advantages and disadvantages. By combining these methods, we can try to mitigate the disadvantages 
of one method by using the other. Some of the combinations for the models that we can use are:
\begin{itemize}
    \item $\alpha$ = 1e-4; Dropout = 0.75; L1 Penalty = 5e-4
    \item $\alpha$ = 1e-4; Dropout = 0.97; L1 Penalty = 1e-3
    \item $\alpha$ = 1e-4; Dropout = 0.75; L2 Penalty = 5e-4
    \item $\alpha$ = 5e-4; Dropout = 0.97; L2 Penalty = 1e-3
    \item $\alpha$ = 1e-4; L1 Penalty = 5e-4; L2 Penalty = 5e-4
    \item $\alpha$ = 1e-4; L1 Penalty = 3e-4; L2 Penalty = 9e-4
    \item $\alpha$ = 1e-4; L1 Penalty = 5e-4; L2 Penalty = 1e-3; Dropout = 0.9
    \item $\alpha$ = 1e-4; L1 Penalty = 1e-4; L2 Penalty = 9e-4; Dropout = 0.9
\end{itemize}
These experiments would give us an idea about which regularisation methods work well together. The dropouts control the number of neurons the model is using to train. Controlling this directly controls the capacity of the network and hence it is a crucial tool in preventing overfitting. 
\par The L1 norms penalise the model for having too large gradients and bring them down to 0. A combination of L1 norms and Dropouts would promote a simpler model with small architecture which would help with generalisation and faster convergence. L2 norms also penalise the model for having large weights, but instead of reducing the weights to 0, it scales it down
in a uniform scale. Dropouts with L2 will lead to a smoother model which will converge in fewer instances while still helping the performance of the model. The values of hyperparameters above will check well performing regularisation methods in combination with another relatively low performing regulariser to find the ideal balance of the parameters. We found that 
the model with learning rate 1e-4, L1 penalty 5e-4, L2 penalty 1e-3 and dropout inclusion probability of 0.9 performed the best giving an validation accuracy of $83.5\%$}
}



%% Question 15: Briefly draw your conclusions based on the results from the previous sections (what are the take-away messages?), discussing them in the context of the overall literature, and conclude your report with a recommendation for future directions
\newcommand{\questionFifteen} {
\youranswer
{In this report, we have discussed the problem of overfitting in machine learning models. We studied how increasing the width and depth of the model increased the model's ability to learn from the data and found that a model with more than required capacity is guaranteed to overfit the training data eventually. Consequentially, we looked at methods to tackle 
this problem of overfitting. Experiments with dropout regularisation were carried out which help in removing the dependency of the model from certain specific neurons. Considering too many neurons caused the model to overfit whereas considering too few neurons made the model underfit the data. We then looked at L1/L2 norms which penalise the weights of the model 
helping in preventing overfitting. Various experiments were formulated to combine the power of dropout and L1/L2 norms to get the best of all the regularisation methods and help the model performance. These formulations can help us get a good idea about combinations of regularisation methods that will work well together. Further studies should
focus on using combined regularisation methods like Elastic Net to prevent overfitting. Early stopping is another approach that can be explored to combat overfitting in machine learning models.}
}

%% - - - - - - - - - - - - FIGURES - - - - - - - - - - - - 

%% Question Figure 2:
\newcommand{\questionFigureTwo} {
\youranswer{Question Figure 2 - Replace the images in Figure 2 with figures depicting the accuracy and error, training and validation curves for your experiments varying the number of hidden units.
%
\begin{figure}[t]
    \centering
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{figures/fig2_acc.pdf}
        \caption{accuracy by epoch}
        \label{fig:width_acccurves}
    \end{subfigure} 
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/fig2_err.pdf}
        \caption{error by epoch}
        \label{fig:width_errorcurves}
    \end{subfigure} 
    \caption{Training and validation curves in terms of classification accuracy (a) and cross-entropy error (b) on the EMNIST dataset for different network widths.}
    \label{fig:width}
\end{figure} 
}
}

%% Question Figure 3:
\newcommand{\questionFigureThree} {
\youranswer{Question Figure 3 - Replace these images with figures depicting the accuracy and error, training and validation curves for your experiments varying the number of hidden layers.
%
\begin{figure}[t]
    \centering
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{figures/fig3_acc.pdf}
        \caption{accuracy by epoch}
        \label{fig:depth_acccurves}
    \end{subfigure} 
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/fig3_err.pdf}
        \caption{error by epoch}
        \label{fig:depth_errorcurves}
    \end{subfigure} 
    \caption{Training and validation curves in terms of classification accuracy (a) and cross-entropy error (b) on the EMNIST dataset for different network depths.}
    \label{fig:depth}
\end{figure} 
}
}

%% Question Figure 4:
\newcommand{\questionFigureFour} {
\youranswer{Question Figure 4 - Replace these images with figures depicting the Validation Accuracy and Generalisation Gap (difference between validation and training error) for each of the experiment results varying the Dropout inclusion rate, and L1/L2 weight penalty depicted in Table 3 (including any results you have filled in).
%
\begin{figure*}[t]
    \centering
    \begin{subfigure}{.475\linewidth}
        \includegraphics[width=\linewidth]{figures/fig4_dropout.pdf}
        \caption{Accuracy and error by inclusion probability.}
        \label{fig:dropoutrates}
    \end{subfigure} 
    \begin{subfigure}{.475\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/fig4_weight.pdf}
        \caption{Accuracy and error by weight penalty.}
        \label{fig:weightrates}
    \end{subfigure} 
    \caption{Accuracy and error by regularisation strength of each method (Dropout and L1/L2 Regularisation).}
    \label{fig:hp_search}
\end{figure*}
}
}

%% - - - - - - - - - - - - TABLES - - - - - - - - - - - - 

%% Question Table 1:
\newcommand{\questionTableOne} {
\youranswer{
Question Table 1 - Fill in Table 1 with the results from your experiments varying the number of hidden units.
%
\begin{table}[t]
    \centering
    \begin{tabular}{c|ccc}
    \toprule
        \# Hidden Units & Val. Acc. & Train Error & Val. Error\\
    \midrule
         32            &    78.9   &  0.563  &   0.705            \\
         64            &    80.9   &  0.345  &   0.660               \\
         128           &    80.5   &  0.163  &   0.924           \\ 
    \bottomrule
    \end{tabular}
    \caption{Validation accuracy (\%) and training/validation error (in terms of cross-entropy error) for varying network widths on the EMNIST dataset.}
    \label{tab:width_exp}
\end{table}
}
}

%% Question Table 2:
\newcommand{\questionTableTwo} {
\youranswer{
Question Table 2 - Fill in Table 2 with the results from your experiments varying the number of hidden layers.
%
\begin{table}[t]
    \centering
    \begin{tabular}{c|ccc}
    \toprule
        \# Hidden Layers & Val. Acc. & Train Error & Val. Error \\
    \midrule
         1               &      80.5      &   0.163 & 0.924               \\
         2               &      82.1      &   0.095 & 1.463                \\
         3               &      82.3      &   0.089 & 1.672                \\ 
    \bottomrule
    \end{tabular}
    \caption{Validation accuracy (\%) and training/validation error (in terms of cross-entropy error) for varying network depths on the EMNIST dataset.}
    \label{tab:depth_exps}
\end{table}
}
}

%% Question Table 3:
\newcommand{\questionTableThree} {
\youranswer{
Question Table 3 - Fill in Table 3 with the results from your experiments for the missing hyperparameter values for each of L1 regularisation, L2 regularisation, and Dropout (use the values shown on the table).
%
\begin{table*}[t]
    \centering
    \begin{tabular}{c|c|ccc}
    \toprule
        Model    &  Hyperparameter value(s) & Validation accuracy & Train Error & Validation Error \\
    \midrule
    \midrule
        Baseline &  -                    &               0.837 &       0.241 &  0.533          \\
    \midrule
        \multirow{4}*{Dropout}
                 & 0.6                   &  80.7                &      0.549 & 0.593     \\
                 & 0.7 & 82.7 & 0.447 & 0.509  \\
                 & \textit{\textbf{0.85}} & \textit{\textbf{85.1}} &  \textit{\textbf{0.329}} &  \textit{\textbf{0.434}} \\
                 & 0.97 & 85.4 & 0.244 & 0.457  \\
    \midrule
        \multirow{4}*{L1 penalty}
                 & \textit{5e-4} & \textit{79.5} & \textit{0.642} & \textit{0.658} \\
                 & 1e-3 & 75.0 & 0.850 & 0.856 \\
                 & 5e-3 & 2.41 & 3.850 & 3.850 \\
                 & 5e-2 & 2.20 & 3.850 & 3.850 \\
    \midrule
        \multirow{4}*{L2 penalty}  
                 & \textit{5e-4} & \textit{85.1} & \textit{0.306} & \textit{0.460} \\
                 & 1e-3 & 84.8 & 0.362 & 0.460 \\
                 & 5e-3 & 81.3 & 0.586 & 0.607 \\
                 & 5e-2 & 39.2 & 2.258 & 2.256  \\      
    \bottomrule
    \end{tabular}
    \caption{Results of all hyperparameter search experiments. \emph{italics} indicate the best results per series (Dropout, L1 Regularisation, L2 Regularisation) and \textbf{bold} indicates the best overall.}
    \label{tab:hp_search}
\end{table*}
}
}

%% END of YOUR ANSWERS
